diff --git a/megatron/bridge/models/conversion/auto_bridge.py b/megatron/bridge/models/conversion/auto_bridge.py
new file mode 100644
index 000000000..1aacffaaf
--- /dev/null
+++ b/megatron/bridge/models/conversion/auto_bridge.py
@@ -0,0 +1,535 @@
+# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import dataclasses
+from functools import cached_property, partial
+from pathlib import Path
+from typing import Any, Generic, Iterable, List, Optional, Type, TypeVar, Union
+
+import torch.distributed as dist
+import transformers
+from megatron.core.transformer.module import MegatronModule
+from megatron.core.transformer.transformer_config import MLATransformerConfig, TransformerConfig
+from transformers.configuration_utils import PretrainedConfig
+from transformers import AutoModelForCausalLM
+from typing_extensions import Unpack
+
+from megatron.bridge.models.conversion import model_bridge
+from megatron.bridge.models.conversion.model_bridge import (
+    HFWeightTuple,
+    MegatronModelBridge,
+    WeightConversionTask,
+)
+from megatron.bridge.models.conversion.utils import get_causal_lm_class_via_auto_map
+#from megatron.bridge.models.gpt_provider import GPTModelProvider
+from megatron.bridge.models.hf_pretrained.causal_lm import PreTrainedCausalLM
+from megatron.bridge.models.hf_pretrained.safe_config_loader import safe_load_config_with_retry
+from megatron.bridge.models.hf_pretrained.state import SafeTensorsStateSource
+#from megatron.bridge.models.model_provider import GetModelKwargs, ModelParallelKwargs, ModelProviderMixin
+
+
+MegatronModelT = TypeVar("MegatronModelT", bound=MegatronModule)
+DataclassT = TypeVar("DataclassT")
+
+
+class AutoBridge(Generic[MegatronModelT]):
+    """
+    Automatically select and instantiate the appropriate bridge for a model.
+
+    This unified bridge class combines automatic model detection with full bridge
+    functionality for converting models between HuggingFace and Megatron formats.
+    It handles the conversion of causal language models (e.g., GPT, Llama, Phi)
+    between HuggingFace's transformers library format and Megatron-Core's distributed
+    training format. It manages weight mapping, tensor parallelism distribution, and
+    configuration translation.
+
+    The bridge supports both directions of conversion:
+    - HuggingFace → Megatron: For training or inference with Megatron
+    - Megatron → HuggingFace: For saving trained models in HF format
+
+    Args:
+        hf_pretrained: Either a PreTrainedCausalLM instance with loaded model,
+            or a PretrainedConfig for configuration-only operations
+
+    Example:
+        >>> # Load and convert a model to Megatron format
+        >>> bridge = AutoBridge.from_hf_pretrained("meta-llama/Meta-Llama-3-8B")
+        >>> provider = bridge.to_megatron_provider()
+        >>> megatron_model = provider.provide_distributed_model(wrap_with_ddp=False)
+
+        >>> # Export a Megatron model back to HuggingFace format
+        >>> bridge.save_hf_pretrained(megatron_model, "./exported_model")
+
+        >>> # Convert weights with custom settings
+        >>> for name, weight in bridge.export_hf_weights(
+        ...     megatron_model,
+        ...     cpu=True
+        ... ):
+        ...     print(f"Exported {name}: {weight.shape}")
+
+        >>> # Check if a model is supported before loading
+        >>> if AutoBridge.can_handle("microsoft/phi-2"):
+        ...     bridge = AutoBridge.from_hf_pretrained("microsoft/phi-2")
+
+    Note:
+        The bridge automatically detects the model architecture and applies
+        the appropriate weight mappings. Custom architectures require implementing
+        a MegatronModelBridge subclass.
+    """
+
+    def __init__(self, hf_pretrained: PreTrainedCausalLM | PretrainedConfig):
+        if not isinstance(hf_pretrained, (PreTrainedCausalLM, PretrainedConfig)):
+            raise ValueError("hf_pretrained must be a PreTrainedCausalLM or PretrainedConfig instance")
+        self.hf_pretrained: PreTrainedCausalLM | PretrainedConfig = hf_pretrained
+
+    @classmethod
+    def list_supported_models(cls) -> list[str]:
+        """
+        List all model architectures currently supported by the bridge system.
+
+        Returns:
+            List of supported HuggingFace model architecture names
+        """
+        # Get all registered implementations from the dispatch system
+        supported = []
+
+        # Access the dispatch registry to find all registered types
+
+        if hasattr(model_bridge.get_model_bridge, "_exact_types"):
+            for arch_type in model_bridge.get_model_bridge._exact_types.keys():
+                # Support both type and string registrations
+                if isinstance(arch_type, str):
+                    supported.append(arch_type)
+                elif hasattr(arch_type, "__name__"):
+                    supported.append(arch_type.__name__)
+
+        return sorted(supported)
+
+    @classmethod
+    def supports(cls, config: Any) -> bool:
+        """
+        Check if this bridge supports the given model configuration.
+
+        A model is supported if it has at least one architecture ending with 'ForCausalLM' or 'ForConditionalGeneration' 
+        or 'NemotronH_Nano_VL_V2'.
+
+        Args:
+            config: HuggingFace model config object
+
+        Returns:
+            True if this bridge can handle the model, False otherwise
+        """
+        architectures = getattr(config, "architectures", [])
+        if not architectures:
+            return False
+        return any(
+            arch.endswith(("ForCausalLM", "ForConditionalGeneration", "NemotronH_Nano_VL_V2")) for arch in architectures
+        )
+
+    @classmethod
+    def from_hf_config(cls, config: PretrainedConfig) -> "AutoBridge":
+        """
+        Create an AutoBridge from a HuggingFace configuration.
+
+        This method creates a bridge instance from just a model configuration,
+        without loading any weights. This is useful for:
+        - Creating Megatron models with random initialization
+        - Working with model architectures without downloading weights
+        - Testing and development scenarios
+
+        Args:
+            config: HuggingFace PretrainedConfig instance containing model
+                architecture information
+
+        Returns:
+            AutoBridge: Bridge instance configured for the architecture
+
+        Raises:
+            ValueError: If the configuration is not for a supported CausalLM model
+
+        Example:
+            >>> from transformers import AutoConfig
+            >>>
+            >>> # Load just the configuration
+            >>> config = AutoConfig.from_pretrained("meta-llama/Meta-Llama-3-8B")
+            >>>
+            >>> # Create bridge from config (no weights)
+            >>> bridge = AutoBridge.from_hf_config(config)
+            >>>
+            >>> # Create Megatron model with random initialization
+            >>> provider = bridge.to_megatron_provider(load_weights=False)
+            >>> model = provider.provide_distributed_model(wrap_with_ddp=False)
+
+            >>> # Or use for architecture exploration
+            >>> transformer_config = bridge.transformer_config
+            >>> print(f"Hidden size: {transformer_config.hidden_size}")
+            >>> print(f"Num layers: {transformer_config.num_layers}")
+
+        See Also:
+            from_hf_pretrained: Create bridge with loaded weights
+            transformer_config: Access the Megatron TransformerConfig
+        """
+        cls._validate_config(config)
+        model = PreTrainedCausalLM()
+        model.config = config
+        model.model = AutoModelForCausalLM.from_config(model.config)
+        return cls(model)
+        #return cls(config)
+
+    @classmethod
+    def from_hf_pretrained(cls, path: Union[str, Path], **kwargs) -> "AutoBridge":
+        """
+        Load an AutoBridge from a pretrained model, automatically detecting the model type.
+
+        This method loads a model from HuggingFace Hub or a local directory and
+        creates a bridge instance ready for conversion operations. The model
+        architecture is validated to ensure compatibility.
+
+        Args:
+            path: HuggingFace model ID or path to model directory
+                Examples: "meta-llama/Meta-Llama-3-8B", "./my_model"
+            **kwargs: Additional arguments passed to HuggingFace from_hf_pretrained
+                Common options include:
+                - torch_dtype: Model precision (torch.float16, torch.bfloat16)
+                - device_map: Device placement strategy ("auto", "cuda:0", etc.)
+                - trust_remote_code: Allow custom model code execution
+                - attn_implementation: Attention implementation ("flash_attention_2", etc.)
+
+        Returns:
+            AutoBridge: Bridge instance with loaded model
+
+        Raises:
+            ValueError: If the model architecture is not supported
+
+        Example:
+            >>> # Basic loading
+            >>> bridge = AutoBridge.from_hf_pretrained("gpt2")
+
+            >>> # Load with specific settings
+            >>> bridge = AutoBridge.from_hf_pretrained(
+            ...     "meta-llama/Meta-Llama-3-8B",
+            ...     torch_dtype=torch.float16,
+            ...     device_map="auto"
+            ... )
+
+            >>> # Works with local paths too
+            >>> bridge = AutoBridge.from_hf_pretrained("/path/to/model")
+        """
+        # First load just the config to check architecture support
+        # Use thread-safe config loading to prevent race conditions
+        config = safe_load_config_with_retry(path, trust_remote_code=kwargs.get("trust_remote_code", False))
+
+        cls._validate_config(config, str(path))
+
+        try:
+            return cls(PreTrainedCausalLM.from_pretrained(path, **kwargs))
+        except Exception as e:
+            raise ValueError(f"Failed to load model with AutoBridge: {e}") from e
+
+
+    def load_hf_weights(self, model: list[MegatronModelT], hf_path: str | Path | None = None) -> None:
+        """
+        Load HuggingFace weights into a Megatron model.
+
+        This method handles the conversion and distribution of weights from
+        HuggingFace format to Megatron's distributed format, including proper
+        tensor parallel and pipeline parallel distribution.
+
+        Args:
+            model: List of Megatron model instances (one per virtual pipeline stage)
+            hf_path: Optional path to load weights from. If None, uses weights
+                from the bridge's hf_pretrained instance
+
+        Returns:
+            The input model with loaded weights
+
+        Raises:
+            ValueError: If hf_path is None and bridge was created without weights
+
+        Example:
+            >>> # Load weights from bridge's pretrained model
+            >>> bridge = AutoBridge.from_hf_pretrained("gpt2")
+            >>> megatron_model = create_megatron_model()  # Your model creation
+            >>> bridge.load_hf_weights(megatron_model)
+
+            >>> # Load weights from a different checkpoint
+            >>> bridge.load_hf_weights(megatron_model, "./finetuned_model")
+        """
+        if hf_path is None:
+            if not isinstance(self.hf_pretrained, PreTrainedCausalLM):
+                raise ValueError("hf_path is required when hf_pretrained is not a PreTrainedCausalLM instance")
+            pre_trained = self.hf_pretrained
+        else:
+            pre_trained = PreTrainedCausalLM.from_pretrained(hf_path)
+            # Preserve trust_remote_code setting from the original bridge instance
+            trust_remote_code = getattr(self.hf_pretrained, 'trust_remote_code', False)
+            pre_trained = PreTrainedCausalLM.from_pretrained(
+                hf_path, 
+                trust_remote_code=trust_remote_code
+            )
+        #self._model_bridge.load_weights_hf_to_megatron(model, pre_trained)
+        self._model_bridge.load_weights_hf_to_megatron(pre_trained,model)#chaixn
+
+        return model
+
+    def save_hf_pretrained(self, model: list[MegatronModelT], path: str | Path, show_progress: bool = True, strict: bool = True) -> None:
+        """
+        Save a Megatron model in HuggingFace format.
+
+        This method exports the complete model including configuration, tokenizer,
+        and weights to a directory that can be loaded with HuggingFace's
+        from_pretrained methods.
+
+        If the original model was loaded with trust_remote_code=True, any custom
+        modeling files (e.g., modeling_*.py, configuration_*.py) will be preserved
+        to ensure the saved model can be loaded properly.
+
+        Args:
+            model: Megatron model instance or list of instances
+            path: Directory path to save the model
+            show_progress: Display progress bar during weight export
+
+        Example:
+            >>> # Save model after training
+            >>> bridge.save_hf_pretrained(megatron_model, "./my_finetuned_model")
+
+            >>> # Load the saved model with HuggingFace
+            >>> from transformers import AutoModelForCausalLM
+            >>> hf_model = AutoModelForCausalLM.from_pretrained("./my_finetuned_model")
+
+        Note:
+            This method is collective - all ranks must call it. Only rank 0
+            saves the configuration files, while weight saving is coordinated
+            across all ranks.
+        """
+        if dist.is_available() and dist.is_initialized():
+            # Distributed training, only rank 0 saves artifacts
+            if dist.get_rank() == 0:
+                self.hf_pretrained.save_artifacts(path)
+        else:
+            # No distributed training, save artifacts
+            self.hf_pretrained.save_artifacts(path)
+
+        self.save_hf_weights(model, path, show_progress, strict)
+
+    def save_hf_weights(self, model: list[MegatronModelT], path: str | Path, show_progress: bool = True, strict: bool = True) -> None:
+        """
+        Save Megatron model weights in HuggingFace safetensors format.
+
+        This method exports only the model weights (not configuration or tokenizer)
+        to safetensors files compatible with HuggingFace. It uses streaming save
+        to handle large models efficiently without requiring all weights in memory
+        at once.
+
+        The weights are gathered from distributed ranks and saved in the standard
+        HuggingFace sharded format when the model is large.
+
+        Args:
+            model: Megatron model instance or list of instances
+            path: Directory path where weight files will be saved
+            show_progress: Display progress bar during export
+
+        Raises:
+            ValueError: If the state source doesn't support streaming save
+
+        Example:
+            >>> # Save just the weights
+            >>> bridge.save_hf_weights(megatron_model, "./model_weights")
+
+            >>> # Save without progress bar (useful in scripts)
+            >>> bridge.save_hf_weights(megatron_model, "./weights", show_progress=False)
+
+        Note:
+            - This method is collective and must be called by all ranks
+            - Uses safetensors format for efficient loading and security
+            - Automatically handles model sharding for large models
+            - The saved weights can be loaded with HuggingFace's from_pretrained
+        """
+        if dist.is_available() and dist.is_initialized():
+            dist.barrier()
+        dispatch_instance = (self._causal_lm_architecture, self._get_model_instance(model))
+        generator = model_bridge.stream_weights_megatron_to_hf(
+            dispatch_instance, model, self.hf_pretrained, cpu=True, show_progress=show_progress
+        )
+        source=SafeTensorsStateSource(path) 
+        # Check if the state source is SafeTensorsStateSource for streaming save.
+        if (
+            hasattr(self.hf_pretrained, "state")
+            and hasattr(self.hf_pretrained.state, "source")
+            #and isinstance(self.hf_pretrained.state.source, SafeTensorsStateSource)
+        ):
+            #self.hf_pretrained.state.source.save_generator(generator, path, strict=strict)
+            source.save_generator(generator, path, strict=strict)
+        else:
+            raise ValueError("The state source is not a SafeTensorsStateSource, cannot save in streaming mode.")
+
+        if dist.is_available() and dist.is_initialized():
+            dist.barrier()
+
+    @property
+    def _model_bridge(self) -> "MegatronModelBridge":
+        return model_bridge.get_model_bridge(self._causal_lm_architecture)
+
+    @cached_property
+    def _causal_lm_architecture(self):
+        """Resolve the model's CausalLM architecture for dispatch.
+
+        Behavior:
+        - If the model can be imported from transformers directly, return the actual transformers class object.
+        - Otherwise, if the model uses HuggingFace auto_map, return the architecture's class name as a string (e.g., 
+        "DeepseekV2ForCausalLM").
+
+        Returns:
+            str | type: The transformers class for the CausalLM architecture or the architecture's class name as a 
+            string for auto_map models.
+
+        Raises:
+            ValueError: If no CausalLM architecture is found or cannot be resolved.
+        """
+        if isinstance(self.hf_pretrained, PreTrainedCausalLM):
+            config = self.hf_pretrained.config
+            model_name_or_path = getattr(config, "_name_or_path", None) or getattr(
+                self.hf_pretrained, "model_name_or_path", None
+            )
+        else:
+            config = self.hf_pretrained
+            model_name_or_path = getattr(config, "_name_or_path", None)
+
+        architectures = getattr(config, "architectures", [])
+
+        if not architectures:
+            raise ValueError(
+                "\n✗ No architectures found in model config\n\n"
+                "The model configuration does not specify any architectures.\n"
+                "This is required for determining the model type."
+            )
+
+        causal_lm_arch = None
+        for architecture_name in architectures:
+            # TODO: Can we improve this?
+            if architecture_name.endswith(("ForCausalLM", "ForConditionalGeneration", "NemotronH_Nano_VL_V2")):
+                causal_lm_arch = architecture_name
+                break
+
+        if not causal_lm_arch:
+            raise ValueError(
+                f"\n✗ No CausalLM architecture found\n\n"
+                f"Model architectures: {architectures}\n\n"
+                f"None of the architectures end with 'ForCausalLM' or 'ForConditionalGeneration' or"
+                f"'NemotronH_Nano_VL_V2'.\n"
+                f"This bridge only supports causal language models.\n"
+                f"For other model types, use a different bridge class."
+            )
+
+        # Try auto_map first
+        cls = get_causal_lm_class_via_auto_map(model_name_or_path=model_name_or_path, config=config)
+        if cls is not None:
+            # For auto_map models, return the class name as a string
+            return getattr(cls, "__name__", str(cls))
+
+        try:
+            return getattr(transformers, causal_lm_arch)
+        except AttributeError:
+            raise ValueError(
+                f"\n✗ Architecture class '{causal_lm_arch}' not found in transformers\n\n"
+                f"This could mean:\n"
+                f"1. The model requires a newer version of transformers\n"
+                f"2. The model uses a custom modeling file not in the standard library\n"
+                f"3. There's a typo in the architecture name\n\n"
+                f"Please verify your transformers installation and the model requirements."
+            )
+
+    @classmethod
+    def _validate_config(cls, config: PretrainedConfig, path: str | None = None) -> None:
+        # Check if this is a causal LM model
+        if not cls.supports(config):
+            architectures = getattr(config, "architectures", [])
+            raise ValueError(
+                f"\n✗ Model architecture not supported by AutoBridge\n\n"
+                f"Model: {path}\n"
+                f"Architectures: {architectures}\n\n"
+                f"AutoBridge only supports models with architectures ending in 'ForCausalLM' or"
+                f"'ForConditionalGeneration' or 'NemotronH_Nano_VL_V2'.\n"
+                f"Found architectures that don't match this pattern.\n\n"
+                f"If this is a different model type (e.g., Vision, Sequence-to-Sequence),\n"
+                f"you may need to use a different bridge class."
+            )
+
+        # Check if we have an implementation for this specific architecture
+        architecture = None
+        for arch_name in config.architectures:
+            if arch_name.endswith(("ForCausalLM", "ForConditionalGeneration", "NemotronH_Nano_VL_V2")):
+                architecture = arch_name
+                break
+
+        if architecture:
+            # Try auto_map first
+            arch_class = get_causal_lm_class_via_auto_map(model_name_or_path=path, config=config) if path else None
+            if arch_class is not None:
+                # For auto_map models, use class-name string
+                arch_key = getattr(arch_class, "__name__", str(arch_class))
+            else:
+                try:
+                    arch_class = getattr(transformers, architecture)
+                    arch_key = arch_class
+                except AttributeError:
+                    # Fall back to name-based registration
+                    arch_key = architecture
+
+            # Test if we have a registered implementation (type or class-name string)
+            has_implementation = False
+            if hasattr(model_bridge.get_model_bridge, "_exact_types"):
+                registry = model_bridge.get_model_bridge._exact_types
+                if isinstance(arch_key, str):
+                    has_implementation = arch_key in registry
+                else:
+                    has_implementation = (arch_key in registry) or (getattr(arch_key, "__name__", None) in registry)
+
+                if not has_implementation:
+                    # Get list of supported models
+                    supported_models = cls.list_supported_models()
+
+                    raise ValueError(
+                        f"\n✗ Model architecture '{architecture}' is not yet supported\n\n"
+                        f"Model: {path}\n"
+                        f"Architecture: {architecture}\n\n"
+                        f"Currently supported architectures:\n"
+                        + "\n".join(f"  • {model}" for model in supported_models)
+                        + f"\n\nTo add support for {architecture}, you need to:\n"
+                        f"1. Create a new bridge class that inherits from MegatronModelBridge\n"
+                        f"2. Implement the required methods (provider_bridge, mapping_registry)\n"
+                        f"3. Register it with @MegatronModelBridge.register_bridge decorator\n\n"
+                        f"Example implementation:\n"
+                        f"  from megatron.bridge.models.conversion.model_bridge import MegatronModelBridge\n"
+                        f"  from transformers import {architecture}\n"
+                        f"  from megatron.core.models.gpt import GPTModel\n\n"
+                        f"  @MegatronModelBridge.register_bridge(source={architecture}, target=GPTModel)\n"
+                        f"  class Megatron{architecture.replace('ForCausalLM', '')}Bridge(MegatronModelBridge):\n"
+                        f"      def provider_bridge(self, hf_pretrained):\n"
+                        f"          # Return a ModelProvider instance\n"
+                        f"          ...\n\n"
+                        f"      def mapping_registry(self):\n"
+                        f"          # Return a MegatronMappingRegistry with weight mappings\n"
+                        f"          ...\n\n"
+                        f"For reference implementations, see:\n"
+                        f"  • src/megatron/bridge/models/llama/llama_bridge.py\n"
+                        f"  • src/megatron/bridge/models/qwen/qwen_2_causal_bridge.py"
+                    ) from None
+
+    def _get_model_instance(self, model: list[MegatronModelT]) -> MegatronModelT:
+        model_instance = model[0]
+        while hasattr(model_instance, "module"):
+            model_instance = model_instance.module
+        return model_instance
+
