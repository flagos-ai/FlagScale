diff --git a/verl/workers/actor/megatron_actor.py b/verl/workers/actor/megatron_actor.py
index 38d317e1..aa2bdf57 100644
--- a/verl/workers/actor/megatron_actor.py
+++ b/verl/workers/actor/megatron_actor.py
@@ -22,6 +22,7 @@ Note that our model doesn't have to be `MegatronModule` because we don't share e
 import itertools
 import logging
 import os
+import sys
 from functools import partial
 from typing import Iterable
 
@@ -38,7 +39,10 @@ from torch import nn
 
 from verl import DataProto
 from verl.trainer.ppo.core_algos import agg_loss, get_policy_loss_fn, kl_penalty
+from verl.utils import tensordict_utils as tu
 from verl.utils.device import get_device_id, get_torch_device
+from verl.utils.logger import print_rank_0
+from tensordict import TensorDict
 from verl.utils.megatron.pipeline_parallel import make_batch_generator
 from verl.utils.megatron.tensor_parallel import vocab_parallel_entropy, vocab_parallel_log_probs_from_logits
 from verl.utils.megatron_utils import get_model_config
@@ -409,6 +413,43 @@ class MegatronPPOActor(BasePPOActor):
             # We move calculation of entropy to compute_log_probs, forward_only == True
             device = output["log_probs"].device
             metrics = {}
+            
+            # Add unconditional logging to verify loss_func is called
+            # Use both logger.warning (visible with default WARN level) and print to ensure visibility
+            try:
+                global_rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else -1
+                pp_rank = mpu.get_pipeline_model_parallel_rank() if torch.distributed.is_initialized() else -1
+                tp_rank = mpu.get_tensor_model_parallel_rank() if torch.distributed.is_initialized() else -1
+                is_last_stage = mpu.is_pipeline_last_stage(ignore_virtual=True) if torch.distributed.is_initialized() else False
+                msg = f"[MegatronPPOActor] loss_func CALLED: global_rank={global_rank}, pp_rank={pp_rank}, tp_rank={tp_rank}, is_last_stage={is_last_stage}, forward_only={forward_only}"
+                logger.warning(msg)  # Use WARNING level to ensure visibility with default log level
+                print(msg, flush=True, file=sys.stderr)  # Also print to stderr as backup
+            except Exception as e:
+                error_msg = f"[MegatronPPOActor] loss_func CALLED but error getting rank info: {e}"
+                logger.error(error_msg)
+                print(error_msg, flush=True, file=sys.stderr)
+            
+            # Check if sequence packing loss is enabled from config (check early for debugging)
+            log_prob_raw = output["log_probs"]
+            # Safely access policy_loss config (may not exist for ref worker)
+            if hasattr(self.config, "policy_loss") and self.config.policy_loss is not None:
+                seqpack_loss_enabled = getattr(self.config.policy_loss, "seqpack_loss", False)
+            else:
+                seqpack_loss_enabled = False
+            
+            # Log debug info to verify code execution and config loading
+            # Use pipeline last stage rank 0 for logging (loss_func only executes on last stage)
+            if mpu.is_pipeline_last_stage(ignore_virtual=True):
+                if mpu.get_tensor_model_parallel_rank() == 0:
+                    global_rank = torch.distributed.get_rank()
+                    msg = f"[MegatronPPOActor] loss_func (rank={global_rank}): forward_only={forward_only}, seqpack_loss_enabled={seqpack_loss_enabled}"
+                    logger.warning(msg)  # Use WARNING level to ensure visibility
+                    print(msg, flush=True)  # Also print to stdout
+                    if hasattr(self.config, "policy_loss") and self.config.policy_loss is not None:
+                        config_msg = f"[MegatronPPOActor] policy_loss config exists: {self.config.policy_loss}"
+                        logger.warning(config_msg)
+                        print(config_msg, flush=True)
+            
             if forward_only:
                 if post_process_fn is None:
                     pass
@@ -419,76 +460,318 @@ class MegatronPPOActor(BasePPOActor):
                 if not calculate_entropy:
                     return torch.tensor(1.0, device=device), metrics
 
-            responses = data["responses"]
-            response_length = responses.size(1)
-            response_mask = data["response_mask"].to(bool)
-            loss_agg_mode = self.config.loss_agg_mode
-            # compute policy loss
-            log_prob = output["log_probs"][:, -response_length - 1 : -1].contiguous()
-            ret_entropy = None
-            stats = {}
-            if not forward_only:
-                old_log_prob = data["old_log_probs"]
-                advantages = data["advantages"]
-
-                entropy_coeff = self.config.entropy_coeff
+            # Helper function for standard loss computation
+            def compute_standard_loss():
+                """Compute standard PPO loss (non-sequence-packing mode)"""
+                responses = data["responses"]
+                response_length = responses.size(1)
+                response_mask = data["response_mask"].to(bool)
                 loss_agg_mode = self.config.loss_agg_mode
-
-                loss_mode = self.config.policy_loss.get("loss_mode", "vanilla")
-
-                policy_loss_fn = get_policy_loss_fn(loss_mode)
-
-                # Extract pre-computed rollout importance sampling weights if present
-                # Weights are computed centrally in trainer and added when algorithm.rollout_is=True
-                rollout_is_weights = data.get("rollout_is_weights", None)
-
-                # NOTE: Both mismatch diagnostic metrics (PPL, KL, etc.) and IS weight metrics
-                # are computed centrally in ray_trainer.py for consistency and efficiency.
-                # This ensures metrics are computed uniformly across all batches at the trainer level
-                # and avoids redundant computation across workers and micro-batches.
-                pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(
-                    old_log_prob=old_log_prob,
-                    log_prob=log_prob,
-                    advantages=advantages,
-                    response_mask=response_mask,
-                    loss_agg_mode=loss_agg_mode,
-                    config=self.config,
-                    rollout_is_weights=rollout_is_weights,
-                )
-
-                stats.update(
-                    {
+                
+                # Extract response part from log_probs
+                log_prob = output["log_probs"][:, -response_length - 1 : -1].contiguous()
+                ret_entropy = None
+                stats = {}
+                
+                if not forward_only:
+                    old_log_prob = data["old_log_probs"]
+                    advantages = data["advantages"]
+                    
+                    loss_mode = getattr(self.config.policy_loss, "loss_mode", "vanilla") if hasattr(self.config, "policy_loss") and self.config.policy_loss is not None else "vanilla"
+                    policy_loss_fn = get_policy_loss_fn(loss_mode)
+                    rollout_is_weights = data.get("rollout_is_weights", None)
+                    
+                    # Compute policy loss
+                    pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(
+                        old_log_prob=old_log_prob,
+                        log_prob=log_prob,
+                        advantages=advantages,
+                        response_mask=response_mask,
+                        loss_agg_mode=loss_agg_mode,
+                        config=self.config,
+                        rollout_is_weights=rollout_is_weights,
+                    )
+                    
+                    stats.update({
                         "actor/pg_loss": pg_loss.detach().item(),
                         "actor/pg_clipfrac": pg_clipfrac.detach().item(),
                         "actor/ppo_kl": ppo_kl.detach().item(),
                         "actor/pg_clipfrac_lower": pg_clipfrac_lower.detach().item(),
-                    }
-                )
-                policy_loss = pg_loss
-
-            if calculate_entropy:
-                entropy = output["entropy"][:, -response_length - 1 : -1].contiguous()
-                if not forward_only:
-                    entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
-                    entropy_coeff = meta_info["entropy_coeff"]
-                    policy_loss = pg_loss - entropy_coeff * entropy_loss
+                    })
+                    policy_loss = pg_loss
+                    
+                    # Add entropy loss
+                    if calculate_entropy:
+                        entropy = output["entropy"][:, -response_length - 1 : -1].contiguous()
+                        entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
+                        entropy_coeff = meta_info["entropy_coeff"]
+                        policy_loss = pg_loss - entropy_coeff * entropy_loss
+                    
+                    # Add KL loss
+                    if self.config.use_kl_loss:
+                        ref_log_prob = data["ref_log_prob"]
+                        kld = kl_penalty(logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type)
+                        kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=self.config.loss_agg_mode)
+                        policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
+                        stats["actor/kl_loss"] = kl_loss.detach().item()
+                        stats["actor/kl_coef"] = self.config.kl_loss_coef
                 else:
-                    ret_entropy = entropy
-
-            if forward_only:
-                policy_loss = torch.tensor(1.0, device=device)
+                    policy_loss = torch.tensor(1.0, device=device)
+                    if calculate_entropy:
+                        ret_entropy = output["entropy"][:, -response_length - 1 : -1].contiguous()
+                
+                return policy_loss, stats, ret_entropy
+            
+            # Check if sequence packing loss should be used
+            if seqpack_loss_enabled:
+                # Get packed_seq_params from data
+                packed_seq_params = tu.get_non_tensor_data(data, key="packed_seq_params", default=None)
+                
+                # Print condition checks for debugging
+                has_packed_seq_params = packed_seq_params is not None
+                has_is_nested_attr = hasattr(log_prob_raw, "is_nested")
+                is_nested_tensor = has_is_nested_attr and log_prob_raw.is_nested if has_is_nested_attr else False
+                
+                # Use pipeline last stage rank 0 for printing
+                if mpu.is_pipeline_last_stage(ignore_virtual=True) and mpu.get_tensor_model_parallel_rank() == 0:
+                    global_rank = torch.distributed.get_rank()
+                    print(f"[MegatronPPOActor] seqpack_loss=True condition checks (rank={global_rank}):", flush=True)
+                    print(f"  - packed_seq_params is not None: {has_packed_seq_params}", flush=True)
+                    print(f"  - log_prob_raw has 'is_nested' attribute: {has_is_nested_attr}", flush=True)
+                    if has_is_nested_attr:
+                        print(f"  - log_prob_raw.is_nested: {is_nested_tensor}", flush=True)
+                    print(f"  - Will use sequence packing loss: {has_packed_seq_params}", flush=True)
+                
+                # Use sequence packing loss if packed_seq_params is available
+                # Note: Even if log_prob_raw is padded tensor (not nested), we can still use packed_seq_params
+                # to extract sequence boundaries and compute loss per sequence
+                if has_packed_seq_params:
+                    if mpu.is_pipeline_last_stage(ignore_virtual=True) and mpu.get_tensor_model_parallel_rank() == 0:
+                        global_rank = torch.distributed.get_rank()
+                        tensor_type = "nested" if is_nested_tensor else "padded"
+                        print(f"[MegatronPPOActor] Using sequence packing mode for loss computation (seqpack_loss=True, tensor_type={tensor_type}) (rank={global_rank})", flush=True)
+                    # Get sequence boundaries from PackedSeqParams (similar to NeMo-RL)
+                    cu_seqlens_q = packed_seq_params.cu_seqlens_q
+                    cu_seqlens_q_padded = packed_seq_params.cu_seqlens_q_padded
+                    
+                    # Calculate unpadded and padded sequence lengths
+                    unpadded_seq_lengths = cu_seqlens_q[1:] - cu_seqlens_q[:-1]
+                    if cu_seqlens_q_padded is not None:
+                        padded_cu_seqlens = cu_seqlens_q_padded
+                        padded_seq_lengths = cu_seqlens_q_padded[1:] - cu_seqlens_q_padded[:-1]
+                    else:
+                        padded_cu_seqlens = cu_seqlens_q
+                        padded_seq_lengths = unpadded_seq_lengths
+                    
+                    # Get sequence start and end positions
+                    seq_starts = padded_cu_seqlens[:-1]
+                    seq_ends = padded_cu_seqlens[1:]
+                    num_sequences = len(seq_starts)
+                    
+                    loss_accum = 0.0
+                    metrics_accum = {}
+                    
+                    log_prob = log_prob_raw
+                    entropy = output.get("entropy", None)
+                    
+                    # Get response_length from data for extracting response part
+                    responses = data.get("responses", None)
+                    if responses is not None:
+                        response_length = responses.size(1) if len(responses.shape) > 1 else 1
+                    else:
+                        # Fallback: use response_mask to determine response length
+                        response_mask = data.get("response_mask", None)
+                        if response_mask is not None:
+                            response_length = response_mask.size(1) if len(response_mask.shape) > 1 else 1
+                        else:
+                            # Last resort: use log_prob shape
+                            response_length = log_prob.size(1) if len(log_prob.shape) > 1 else 1
+                    
+                    # Handle both nested tensor and padded tensor cases
+                    if is_nested_tensor:
+                        log_prob_values = log_prob.values()    # [total_tokens]
+                        log_prob_offsets = log_prob.offsets()  # cu_seqlens (should match cu_seqlens_q)
+                    else:
+                        # For padded tensor, first extract response part (similar to standard mode)
+                        # log_prob_raw is [batch_size, seq_len], extract response part
+                        log_prob = log_prob[:, -response_length - 1 : -1].contiguous()  # [batch_size, response_length]
+                        log_prob_values = None  # Will extract per sequence
+                        log_prob_offsets = cu_seqlens_q  # Use cu_seqlens_q for sequence boundaries
+                    
+                    # Process each sequence individually (similar to NeMo-RL's wrapper)
+                    for seq_idx in range(num_sequences):
+                        seq_start = seq_starts[seq_idx].item()
+                        seq_end = seq_ends[seq_idx].item()
+                        unpadded_seq_len = unpadded_seq_lengths[seq_idx].item()
+                        
+                        # Get actual response length for this sequence from response_mask
+                        response_mask_seq = data.get("response_mask", None)
+                        if response_mask_seq is not None:
+                            if hasattr(response_mask_seq, "is_nested") and response_mask_seq.is_nested:
+                                mask_offsets = response_mask_seq.offsets()
+                                mask_values = response_mask_seq.values()
+                                mask_start = mask_offsets[seq_idx].item()
+                                mask_end = mask_offsets[seq_idx + 1].item()
+                                actual_response_len = mask_values[mask_start:mask_end].sum().item()
+                            else:
+                                actual_response_len = response_mask_seq[seq_idx].sum().item()
+                        else:
+                            # Fallback: use unpadded_seq_len (but this is total seq len, not response len)
+                            actual_response_len = response_length
+                        
+                        # Extract log_prob for current sequence
+                        if is_nested_tensor:
+                            # Extract from nested tensor
+                            seq_log_prob_start = log_prob_offsets[seq_idx].item()
+                            seq_log_prob_end = log_prob_offsets[seq_idx + 1].item()
+                            seq_log_prob = log_prob_values[seq_log_prob_start:seq_log_prob_end]
+                            seq_log_prob = seq_log_prob.unsqueeze(0)  # [1, seq_len]
+                        else:
+                            # Extract from padded tensor - get response part for this sequence
+                            # log_prob is [batch_size, response_length], extract actual response length
+                            seq_log_prob = log_prob[seq_idx:seq_idx+1, :actual_response_len]  # [1, actual_response_len]
+                        
+                        # Extract data for current sequence and remove padding
+                        # (Similar to NeMo-RL's data.slice() and unpadded_seq_data)
+                        seq_data = {}
+                        for key in ["old_log_probs", "advantages", "response_mask", "ref_log_prob", "responses"]:
+                            if key in data:
+                                tensor = data[key]
+                                if isinstance(tensor, torch.Tensor):
+                                    # Handle nested tensor
+                                    if hasattr(tensor, "is_nested") and tensor.is_nested:
+                                        tensor_offsets = tensor.offsets()
+                                        tensor_values = tensor.values()
+                                        tensor_start = tensor_offsets[seq_idx].item()
+                                        tensor_end = tensor_offsets[seq_idx + 1].item()
+                                        seq_tensor = tensor_values[tensor_start:tensor_end].unsqueeze(0)
+                                        seq_data[key] = seq_tensor
+                                    # Handle regular tensor - extract response part for response-related tensors
+                                    elif tensor.ndim > 1 and tensor.shape[1] > 1:
+                                        # For response-related tensors (old_log_probs, advantages, etc.), 
+                                        # they are already in response format [batch_size, response_length]
+                                        # Extract the actual response length for this sequence
+                                        if key in ["old_log_probs", "advantages", "ref_log_prob"]:
+                                            # These are response-length tensors, use actual_response_len
+                                            seq_data[key] = tensor[seq_idx:seq_idx+1, :actual_response_len]
+                                        else:
+                                            # For other tensors, use unpadded_seq_len
+                                            seq_data[key] = tensor[seq_idx:seq_idx+1, :unpadded_seq_len]
+                                    else:
+                                        seq_data[key] = tensor[seq_idx:seq_idx+1]
+                                else:
+                                    seq_data[key] = tensor
+                        
+                        # Create model_output for current sequence
+                        seq_model_output = {"log_probs": seq_log_prob}
+                        if entropy is not None:
+                            if hasattr(entropy, "is_nested") and entropy.is_nested:
+                                entropy_offsets = entropy.offsets()
+                                entropy_values = entropy.values()
+                                entropy_start = entropy_offsets[seq_idx].item()
+                                entropy_end = entropy_offsets[seq_idx + 1].item()
+                                seq_entropy = entropy_values[entropy_start:entropy_end].unsqueeze(0)
+                            else:
+                                # entropy is also in response format, extract response part first
+                                entropy_response = entropy[:, -response_length - 1 : -1].contiguous() if entropy.shape[1] > response_length else entropy
+                                seq_entropy = entropy_response[seq_idx:seq_idx+1, :actual_response_len]
+                            seq_model_output["entropy"] = seq_entropy
+                        
+                        # Create TensorDict for current sequence
+                        seq_data_dict = TensorDict(seq_data, batch_size=[1])
+                        # Copy non-tensor metadata
+                        for key in data.keys():
+                            if key not in seq_data and not isinstance(data[key], torch.Tensor):
+                                # Get non-tensor data with default value None
+                                non_tensor_value = tu.get_non_tensor_data(data, key, default=None)
+                                if non_tensor_value is not None:
+                                    tu.assign_non_tensor_data(seq_data_dict, key, non_tensor_value)
+                        
+                        # Get response_mask for current sequence
+                        if "response_mask" not in seq_data_dict:
+                            if "response_mask" in data:
+                                response_mask_tensor = data["response_mask"]
+                                if hasattr(response_mask_tensor, "is_nested") and response_mask_tensor.is_nested:
+                                    mask_offsets = response_mask_tensor.offsets()
+                                    mask_values = response_mask_tensor.values()
+                                    mask_start = mask_offsets[seq_idx].item()
+                                    mask_end = mask_offsets[seq_idx + 1].item()
+                                    seq_response_mask = mask_values[mask_start:mask_end].unsqueeze(0)
+                                else:
+                                    # response_mask is in response format, use actual_response_len
+                                    seq_response_mask = response_mask_tensor[seq_idx:seq_idx+1, :actual_response_len]
+                                seq_data_dict["response_mask"] = seq_response_mask
+                        
+                        response_mask = seq_data_dict["response_mask"].to(bool)
+                        
+                        # Compute loss for current sequence using standard logic
+                        # (Similar to NeMo-RL calling self.loss_fn on single sequence)
+                        if not forward_only:
+                            old_log_prob = seq_data_dict["old_log_probs"]
+                            advantages = seq_data_dict["advantages"]
+                            
+                            loss_mode = getattr(self.config.policy_loss, "loss_mode", "vanilla") if hasattr(self.config, "policy_loss") and self.config.policy_loss is not None else "vanilla"
+                            policy_loss_fn = get_policy_loss_fn(loss_mode)
+                            rollout_is_weights = seq_data_dict.get("rollout_is_weights", None)
+                            
+                            # Compute policy loss for this sequence
+                            pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(
+                                old_log_prob=old_log_prob,
+                                log_prob=seq_log_prob,
+                                advantages=advantages,
+                                response_mask=response_mask,
+                                loss_agg_mode=self.config.loss_agg_mode,
+                                config=self.config,
+                                rollout_is_weights=rollout_is_weights,
+                            )
+                            
+                            seq_policy_loss = pg_loss
+                            
+                            # Add entropy loss
+                            if entropy is not None and "entropy" in seq_model_output:
+                                seq_entropy = seq_model_output["entropy"]
+                                entropy_loss = agg_loss(loss_mat=seq_entropy, loss_mask=response_mask, loss_agg_mode=self.config.loss_agg_mode)
+                                entropy_coeff = meta_info["entropy_coeff"]
+                                seq_policy_loss = pg_loss - entropy_coeff * entropy_loss
+                            
+                            # Add KL loss
+                            if self.config.use_kl_loss:
+                                ref_log_prob = seq_data_dict["ref_log_prob"]
+                                kld = kl_penalty(logprob=seq_log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type)
+                                kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=self.config.loss_agg_mode)
+                                seq_policy_loss = seq_policy_loss + kl_loss * self.config.kl_loss_coef
+                            
+                            # Accumulate loss and metrics (similar to NeMo-RL)
+                            loss_accum += seq_policy_loss
+                            metrics_accum["actor/pg_loss"] = metrics_accum.get("actor/pg_loss", 0.0) + pg_loss.detach().item()
+                            metrics_accum["actor/pg_clipfrac"] = metrics_accum.get("actor/pg_clipfrac", 0.0) + pg_clipfrac.detach().item()
+                            metrics_accum["actor/ppo_kl"] = metrics_accum.get("actor/ppo_kl", 0.0) + ppo_kl.detach().item()
+                            metrics_accum["actor/pg_clipfrac_lower"] = metrics_accum.get("actor/pg_clipfrac_lower", 0.0) + pg_clipfrac_lower.detach().item()
+                            if self.config.use_kl_loss:
+                                metrics_accum["actor/kl_loss"] = metrics_accum.get("actor/kl_loss", 0.0) + kl_loss.detach().item()
+                                metrics_accum["actor/kl_coef"] = self.config.kl_loss_coef
+                    
+                    policy_loss = loss_accum
+                    stats = metrics_accum
+                    ret_entropy = None
+                    if calculate_entropy and entropy is not None:
+                        ret_entropy = entropy
+                    
+                    if forward_only:
+                        policy_loss = torch.tensor(1.0, device=device)
+                else:
+                    # seqpack_loss is enabled but data is not in packed format, fall back to standard mode
+                    if mpu.is_pipeline_last_stage(ignore_virtual=True) and mpu.get_tensor_model_parallel_rank() == 0:
+                        global_rank = torch.distributed.get_rank()
+                        print(f"[Warning] seqpack_loss=True is set in config, but data is not in packed format. Falling back to standard loss computation. (rank={global_rank})", flush=True)
+                        print(f"[MegatronPPOActor] Using standard mode for loss computation (seqpack_loss=True but data not packed) (rank={global_rank})", flush=True)
+                    policy_loss, stats, ret_entropy = compute_standard_loss()
             else:
-                if self.config.use_kl_loss:
-                    ref_log_prob = data["ref_log_prob"]
-                    # compute kl loss
-                    kld = kl_penalty(logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type)
-                    kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=self.config.loss_agg_mode)
-
-                    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
-                    metrics["actor/kl_loss"] = kl_loss.detach().item()
-                    metrics["actor/kl_coef"] = self.config.kl_loss_coef
-
-                # return loss and stats
+                # seqpack_loss is False or not set, use standard mode
+                if mpu.is_pipeline_last_stage(ignore_virtual=True) and mpu.get_tensor_model_parallel_rank() == 0:
+                    global_rank = torch.distributed.get_rank()
+                    print(f"[MegatronPPOActor] Using standard mode for loss computation (seqpack_loss=False or not set) (rank={global_rank})", flush=True)
+                policy_loss, stats, ret_entropy = compute_standard_loss()
 
             append_to_dict(metrics, stats)
             return policy_loss, [metrics, ret_entropy]
@@ -530,6 +813,7 @@ class MegatronPPOActor(BasePPOActor):
                     label_mask,
                     temperature,
                     multi_modal_inputs,
+                    data=batch,
                 )
             else:
                 forward_fn = get_mcore_forward_fn(self.hf_config)
@@ -564,6 +848,7 @@ class MegatronPPOActor(BasePPOActor):
                     multi_modal_inputs,
                     logits_processor=logits_processor,
                     logits_processor_args=logits_processor_args,
+                    data=batch,
                 )
 
             if forward_only:
