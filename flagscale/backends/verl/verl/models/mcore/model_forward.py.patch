diff --git a/verl/models/mcore/model_forward.py b/verl/models/mcore/model_forward.py
index b6192386..644837c7 100644
--- a/verl/models/mcore/model_forward.py
+++ b/verl/models/mcore/model_forward.py
@@ -34,6 +34,7 @@ def model_forward_gen(vision_model: bool = False):
         logits_processor=None,
         logits_processor_args: dict = None,
         value_model=False,
+        data=None,
     ):
         """Forward pass for models with sequence packing."""
         pre_process = (
@@ -50,6 +51,11 @@ def model_forward_gen(vision_model: bool = False):
         batch_size, seq_len = attention_mask.shape[:2]
         input_ids_rmpad, packed_seq_params = preprocess_packed_seqs(input_ids, attention_mask, pre_process=pre_process)
         input_ids_rmpad = input_ids_rmpad.contiguous()
+        
+        # Store packed_seq_params in data if provided
+        if data is not None:
+            from verl.utils import tensordict_utils as tu
+            tu.assign_non_tensor(data, packed_seq_params=packed_seq_params)
         output_orig = model(
             input_ids=input_ids_rmpad,
             attention_mask=None,
@@ -87,6 +93,7 @@ def gptmodel_forward_no_padding(
     logits_processor=None,
     logits_processor_args: dict = None,
     value_model=False,
+    data=None,
 ):
     """Default forward pass for GPT models with optional sequence packing."""
     pre_process = unwrap_model(model).pre_process
@@ -101,6 +108,11 @@ def gptmodel_forward_no_padding(
     batch_size = input_ids.shape[0]
     input_ids_rmpad, packed_seq_params = preprocess_packed_seqs_no_padding(input_ids, pre_process=pre_process)
     input_ids_rmpad = input_ids_rmpad.contiguous()
+    
+    if data is not None:
+        from verl.utils import tensordict_utils as tu
+        tu.assign_non_tensor(data, packed_seq_params=packed_seq_params)
+
     output_orig = model(
         input_ids=input_ids_rmpad,
         attention_mask=None,
